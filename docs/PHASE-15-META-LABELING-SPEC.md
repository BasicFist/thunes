# THUNES Phase 15: Meta-Labeling Technical Specification

**Version**: 1.0
**Target Phase**: 15 (Post Phase 14 Micro-Live)
**Expected Timeline**: 4-6 weeks implementation
**Prerequisites**: 1,000+ historical trades from Phases 1-14

---

## Overview

Meta-labeling is a technique to amplify existing trading signals by using ML to filter out low-quality trades. Based on Reddit community consensus and academic research, this approach can improve win rates by 1-3% and reduce drawdowns by 20-40%.

`★ Insight ─────────────────────────────────────`
Meta-labeling doesn't find alpha - it validates and amplifies existing alpha. The primary strategy (SMA crossover) generates signals, while the meta-model acts as a quality filter. This is fundamentally different from trying to predict price movements directly with ML.
`─────────────────────────────────────────────────`

---

## System Architecture

```
┌─────────────────┐     ┌──────────────────┐     ┌─────────────────┐
│ Primary Signal  │────>│  Feature Engine  │────>│  Meta-Labeling  │
│   Generator     │     │                  │     │    Ensemble     │
│  (SMA/RSI)     │     │  (No Lookahead)  │     │   (XGBoost+)    │
└─────────────────┘     └──────────────────┘     └─────────────────┘
         │                       │                         │
         v                       v                         v
┌─────────────────┐     ┌──────────────────┐     ┌─────────────────┐
│  Raw Signals    │     │  43 Features     │     │  Confidence     │
│   (All)         │     │  at Signal Time  │     │  Score (0-1)    │
└─────────────────┘     └──────────────────┘     └─────────────────┘
                                                           │
                                                           v
                                                  ┌─────────────────┐
                                                  │ Trade Decision  │
                                                  │ if conf > 0.6   │
                                                  └─────────────────┘
```

---

## Implementation Components

### 1. Data Collection Pipeline

```python
# src/metalabeling/data_collector.py

from dataclasses import dataclass
from typing import List, Optional
import pandas as pd
import numpy as np

@dataclass
class SignalRecord:
    """Record for every signal generated (executed or not)"""
    timestamp: pd.Timestamp
    symbol: str
    signal_type: str  # 'buy' or 'sell'
    entry_price: float
    features: dict  # All features at signal time
    executed: bool  # Was this signal traded?
    outcome_label: Optional[int] = None  # 1=profitable, 0=unprofitable
    outcome_return: Optional[float] = None
    outcome_duration: Optional[int] = None  # bars until exit

class SignalCollector:
    """
    Collects ALL signals from primary strategy for labeling
    CRITICAL: Must run in parallel with live trading to capture all signals
    """

    def __init__(self, lookback_window: int = 20, profit_threshold: float = 0.002):
        self.signals = []
        self.lookback_window = lookback_window
        self.profit_threshold = profit_threshold

    def record_signal(self, timestamp, symbol, signal_type, price, features, executed=False):
        """Record every signal generated by primary strategy"""
        record = SignalRecord(
            timestamp=timestamp,
            symbol=symbol,
            signal_type=signal_type,
            entry_price=price,
            features=features,
            executed=executed
        )
        self.signals.append(record)

    def label_historical_signals(self, price_data: pd.DataFrame) -> pd.DataFrame:
        """
        Label signals based on future price movement
        This is done on historical data for training
        """
        labeled_signals = []

        for signal in self.signals:
            # Get future price window
            signal_idx = price_data.index.get_loc(signal.timestamp)
            future_window = price_data.iloc[signal_idx:signal_idx + self.lookback_window]

            if len(future_window) < self.lookback_window:
                continue  # Skip incomplete windows

            # Calculate outcome
            if signal.signal_type == 'buy':
                max_return = (future_window['high'].max() - signal.entry_price) / signal.entry_price
                exit_price = future_window['close'].iloc[-1]
                actual_return = (exit_price - signal.entry_price) / signal.entry_price
            else:  # sell
                max_return = (signal.entry_price - future_window['low'].min()) / signal.entry_price
                exit_price = future_window['close'].iloc[-1]
                actual_return = (signal.entry_price - exit_price) / signal.entry_price

            # Binary labeling based on threshold
            signal.outcome_label = 1 if max_return > self.profit_threshold else 0
            signal.outcome_return = actual_return
            signal.outcome_duration = self.lookback_window

            labeled_signals.append(signal)

        return pd.DataFrame([s.__dict__ for s in labeled_signals])
```

### 2. Feature Engineering Module

```python
# src/metalabeling/features.py

class MetaLabelingFeatures:
    """
    Generate features for meta-labeling
    ALL features must be available at signal generation time
    """

    @staticmethod
    def calculate_all_features(df: pd.DataFrame, signal_idx: int) -> dict:
        """
        Calculate all features at signal time
        CRITICAL: Only use data up to signal_idx (no lookahead)
        """
        features = {}

        # Ensure we have enough history
        if signal_idx < 200:
            return None

        historical = df.iloc[:signal_idx].copy()
        current = df.iloc[signal_idx]

        # === Price-Based Features (14 features) ===
        features['rsi_14'] = MetaLabelingFeatures._calculate_rsi(historical['close'], 14)
        features['rsi_14_slope'] = features['rsi_14'] - MetaLabelingFeatures._calculate_rsi(historical['close'].iloc[:-5], 14)
        features['price_to_sma_20'] = current['close'] / historical['close'].rolling(20).mean().iloc[-1]
        features['price_to_sma_50'] = current['close'] / historical['close'].rolling(50).mean().iloc[-1]
        features['price_to_sma_200'] = current['close'] / historical['close'].rolling(200).mean().iloc[-1]
        features['sma_20_50_spread'] = (historical['close'].rolling(20).mean().iloc[-1] -
                                       historical['close'].rolling(50).mean().iloc[-1]) / historical['close'].rolling(50).mean().iloc[-1]

        # MACD
        exp1 = historical['close'].ewm(span=12, adjust=False).mean()
        exp2 = historical['close'].ewm(span=26, adjust=False).mean()
        features['macd'] = (exp1 - exp2).iloc[-1]
        features['macd_signal'] = (exp1 - exp2).ewm(span=9, adjust=False).mean().iloc[-1]
        features['macd_histogram'] = features['macd'] - features['macd_signal']

        # Price momentum
        features['return_1d'] = (current['close'] - historical['close'].iloc[-2]) / historical['close'].iloc[-2]
        features['return_5d'] = (current['close'] - historical['close'].iloc[-6]) / historical['close'].iloc[-6]
        features['return_20d'] = (current['close'] - historical['close'].iloc[-21]) / historical['close'].iloc[-21]

        # Support/Resistance
        features['distance_to_20d_high'] = (historical['high'].rolling(20).max().iloc[-1] - current['close']) / current['close']
        features['distance_to_20d_low'] = (current['close'] - historical['low'].rolling(20).min().iloc[-1]) / current['close']

        # === Volume Features (8 features) ===
        features['volume_ratio'] = current['volume'] / historical['volume'].rolling(20).mean().iloc[-1]
        features['volume_ratio_5d'] = historical['volume'].rolling(5).mean().iloc[-1] / historical['volume'].rolling(20).mean().iloc[-1]

        # On-Balance Volume
        obv = (historical['volume'] * (~historical['close'].diff().le(0) * 2 - 1)).cumsum()
        features['obv_slope'] = (obv.iloc[-1] - obv.iloc[-20]) / abs(obv.iloc[-20]) if obv.iloc[-20] != 0 else 0

        # VWAP
        typical_price = (historical['high'] + historical['low'] + historical['close']) / 3
        vwap = (typical_price * historical['volume']).rolling(20).sum() / historical['volume'].rolling(20).sum()
        features['price_to_vwap'] = current['close'] / vwap.iloc[-1]

        # Volume momentum
        features['volume_change_1d'] = (current['volume'] - historical['volume'].iloc[-2]) / historical['volume'].iloc[-2]
        features['volume_change_5d'] = (historical['volume'].rolling(5).mean().iloc[-1] -
                                       historical['volume'].rolling(5).mean().iloc[-6]) / historical['volume'].rolling(5).mean().iloc[-6]

        # Dollar volume
        features['dollar_volume_ratio'] = (current['close'] * current['volume']) / \
                                         (historical['close'] * historical['volume']).rolling(20).mean().iloc[-1]

        # Volume at price levels
        features['volume_at_high'] = historical[historical['close'] > historical['close'].quantile(0.8)]['volume'].mean() / \
                                    historical['volume'].mean()

        # === Volatility Features (10 features) ===
        features['atr_14'] = MetaLabelingFeatures._calculate_atr(historical, 14)
        features['atr_ratio'] = features['atr_14'] / current['close']
        features['historical_volatility_20'] = historical['close'].pct_change().rolling(20).std().iloc[-1]
        features['historical_volatility_60'] = historical['close'].pct_change().rolling(60).std().iloc[-1]

        # Bollinger Bands
        sma_20 = historical['close'].rolling(20).mean().iloc[-1]
        std_20 = historical['close'].rolling(20).std().iloc[-1]
        features['bb_upper'] = sma_20 + (2 * std_20)
        features['bb_lower'] = sma_20 - (2 * std_20)
        features['bb_width'] = (features['bb_upper'] - features['bb_lower']) / sma_20
        features['bb_position'] = (current['close'] - features['bb_lower']) / (features['bb_upper'] - features['bb_lower'])

        # Volatility ratios
        features['volatility_ratio_short_long'] = features['historical_volatility_20'] / features['historical_volatility_60']
        features['intraday_range'] = (current['high'] - current['low']) / current['close']

        # === Market Structure Features (6 features) ===
        if 'bid' in df.columns and 'ask' in df.columns:
            features['bid_ask_spread'] = (current['ask'] - current['bid']) / current['close']
            features['bid_ask_spread_ma'] = ((df['ask'] - df['bid']) / df['close']).rolling(20).mean().iloc[-1]
        else:
            features['bid_ask_spread'] = 0
            features['bid_ask_spread_ma'] = 0

        # Price levels
        features['close_to_high_ratio'] = (current['close'] - current['low']) / (current['high'] - current['low']) if current['high'] != current['low'] else 0.5
        features['high_low_ratio'] = (current['high'] - current['low']) / current['close']
        features['gap_from_previous_close'] = (current['open'] - historical['close'].iloc[-2]) / historical['close'].iloc[-2]

        # Trend strength
        features['adx_14'] = MetaLabelingFeatures._calculate_adx(historical, 14)

        # === Time-Based Features (5 features) ===
        features['hour'] = signal_idx % 24  # Assuming hourly data
        features['day_of_week'] = pd.Timestamp.now().dayofweek
        features['day_of_month'] = pd.Timestamp.now().day
        features['bars_since_last_signal'] = MetaLabelingFeatures._bars_since_signal(df, signal_idx)
        features['time_in_current_trend'] = MetaLabelingFeatures._time_in_trend(df, signal_idx)

        return features

    @staticmethod
    def _calculate_rsi(prices, period=14):
        """Calculate RSI"""
        delta = prices.diff()
        gain = (delta.where(delta > 0, 0)).rolling(window=period).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(window=period).mean()
        rs = gain / loss
        rsi = 100 - (100 / (1 + rs))
        return rsi.iloc[-1]

    @staticmethod
    def _calculate_atr(df, period=14):
        """Calculate Average True Range"""
        high_low = df['high'] - df['low']
        high_close = (df['high'] - df['close'].shift()).abs()
        low_close = (df['low'] - df['close'].shift()).abs()
        true_range = pd.concat([high_low, high_close, low_close], axis=1).max(axis=1)
        return true_range.rolling(period).mean().iloc[-1]

    @staticmethod
    def _calculate_adx(df, period=14):
        """Calculate ADX (simplified)"""
        # Simplified ADX calculation
        high_diff = df['high'].diff()
        low_diff = -df['low'].diff()
        pos_dm = high_diff.where((high_diff > low_diff) & (high_diff > 0), 0)
        neg_dm = low_diff.where((low_diff > high_diff) & (low_diff > 0), 0)
        atr = MetaLabelingFeatures._calculate_atr(df, period)
        pos_di = 100 * pos_dm.rolling(period).mean() / atr
        neg_di = 100 * neg_dm.rolling(period).mean() / atr
        dx = 100 * abs(pos_di - neg_di) / (pos_di + neg_di)
        adx = dx.rolling(period).mean()
        return adx.iloc[-1] if not pd.isna(adx.iloc[-1]) else 25  # Default to 25

    @staticmethod
    def _bars_since_signal(df, current_idx):
        """Calculate bars since last signal"""
        # Implementation depends on signal storage
        return 10  # Placeholder

    @staticmethod
    def _time_in_trend(df, current_idx):
        """Calculate time in current trend"""
        # Implementation depends on trend definition
        return 20  # Placeholder
```

### 3. Model Training Pipeline

```python
# src/metalabeling/train.py

import numpy as np
import pandas as pd
from sklearn.model_selection import TimeSeriesSplit
from sklearn.preprocessing import StandardScaler
from sklearn.calibration import CalibratedClassifierCV
from sklearn.metrics import classification_report, roc_auc_score
import xgboost as xgb
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
import joblib

class MetaLabelingPipeline:
    """Complete training pipeline for meta-labeling"""

    def __init__(self, min_signals=1000):
        self.min_signals = min_signals
        self.models = {}
        self.scaler = StandardScaler()
        self.feature_columns = None
        self.performance_metrics = {}

    def validate_data(self, df: pd.DataFrame) -> bool:
        """Validate we have enough data for training"""
        n_signals = len(df)
        n_positive = df['outcome_label'].sum()
        n_negative = n_signals - n_positive

        print(f"Total signals: {n_signals}")
        print(f"Positive labels: {n_positive} ({n_positive/n_signals*100:.1f}%)")
        print(f"Negative labels: {n_negative} ({n_negative/n_signals*100:.1f}%)")

        if n_signals < self.min_signals:
            print(f"WARNING: Only {n_signals} signals, need {self.min_signals} minimum")
            return False

        if n_positive < 100 or n_negative < 100:
            print("WARNING: Severe class imbalance")
            return False

        return True

    def prepare_features(self, df: pd.DataFrame) -> tuple:
        """Prepare features and labels for training"""
        # Extract features from the nested dict column
        feature_df = pd.json_normalize(df['features'])
        self.feature_columns = feature_df.columns.tolist()

        # Handle missing values
        feature_df = feature_df.fillna(feature_df.mean())

        # Scale features
        X = self.scaler.fit_transform(feature_df)
        y = df['outcome_label'].values

        return X, y, df['timestamp'].values

    def train_ensemble(self, X, y, timestamps):
        """Train ensemble with purged cross-validation"""
        print("\nTraining ensemble models...")

        # Define base models
        base_models = {
            'xgboost': xgb.XGBClassifier(
                n_estimators=100,
                max_depth=4,
                learning_rate=0.01,
                subsample=0.8,
                colsample_bytree=0.8,
                random_state=42,
                use_label_encoder=False,
                eval_metric='logloss'
            ),
            'random_forest': RandomForestClassifier(
                n_estimators=100,
                max_depth=4,
                min_samples_split=50,
                min_samples_leaf=20,
                random_state=42
            ),
            'logistic': LogisticRegression(
                penalty='l2',
                C=0.1,
                max_iter=1000,
                random_state=42
            )
        }

        # Purged time series cross-validation
        tscv = TimeSeriesSplit(n_splits=5)
        cv_scores = {name: [] for name in base_models.keys()}

        for fold, (train_idx, test_idx) in enumerate(tscv.split(X)):
            print(f"\nFold {fold + 1}/5")

            # Add embargo period (100 samples)
            embargo = 100
            test_start = test_idx[0]
            train_idx = train_idx[train_idx < test_start - embargo]

            if len(train_idx) < 100:
                print(f"Skipping fold {fold + 1} - insufficient training data after embargo")
                continue

            X_train, X_test = X[train_idx], X[test_idx]
            y_train, y_test = y[train_idx], y[test_idx]

            for name, model in base_models.items():
                # Train model
                model.fit(X_train, y_train)

                # Evaluate
                y_pred_proba = model.predict_proba(X_test)[:, 1]
                auc = roc_auc_score(y_test, y_pred_proba)
                cv_scores[name].append(auc)
                print(f"  {name} AUC: {auc:.4f}")

        # Train final models on all data
        for name, model in base_models.items():
            print(f"\nTraining final {name} model...")

            # Calibrate probabilities
            calibrated = CalibratedClassifierCV(model, method='isotonic', cv=3)
            calibrated.fit(X, y)
            self.models[name] = calibrated

            # Report performance
            mean_auc = np.mean(cv_scores[name])
            std_auc = np.std(cv_scores[name])
            print(f"  Cross-validation AUC: {mean_auc:.4f} (+/- {std_auc:.4f})")
            self.performance_metrics[name] = {'mean_auc': mean_auc, 'std_auc': std_auc}

        # Train meta-model (simple averaging for robustness)
        self.train_meta_model(X, y)

    def train_meta_model(self, X, y):
        """Train meta-model to combine base model predictions"""
        print("\nTraining meta-model...")

        # Get base model predictions
        base_predictions = []
        for name, model in self.models.items():
            pred_proba = model.predict_proba(X)[:, 1]
            base_predictions.append(pred_proba)

        # Stack predictions
        X_meta = np.column_stack(base_predictions)

        # Simple weighted average based on CV performance
        weights = []
        for name in self.models.keys():
            weights.append(self.performance_metrics[name]['mean_auc'])

        weights = np.array(weights) / sum(weights)
        print(f"Model weights: {dict(zip(self.models.keys(), weights))}")

        self.meta_weights = weights

    def predict(self, X):
        """Generate ensemble predictions"""
        # Scale features
        X_scaled = self.scaler.transform(X)

        # Get base model predictions
        base_predictions = []
        for name, model in self.models.items():
            pred_proba = model.predict_proba(X_scaled)[:, 1]
            base_predictions.append(pred_proba)

        # Weighted average
        ensemble_prediction = np.average(base_predictions, axis=0, weights=self.meta_weights)

        return ensemble_prediction

    def save_models(self, output_dir='models/metalabeling/'):
        """Save trained models and preprocessors"""
        import os
        os.makedirs(output_dir, exist_ok=True)

        # Save models
        for name, model in self.models.items():
            joblib.dump(model, f"{output_dir}/{name}_model.pkl")

        # Save scaler
        joblib.dump(self.scaler, f"{output_dir}/scaler.pkl")

        # Save meta weights
        np.save(f"{output_dir}/meta_weights.npy", self.meta_weights)

        # Save feature columns
        with open(f"{output_dir}/feature_columns.txt", 'w') as f:
            for col in self.feature_columns:
                f.write(f"{col}\n")

        # Save performance metrics
        pd.DataFrame(self.performance_metrics).to_csv(f"{output_dir}/performance_metrics.csv")

        print(f"\nModels saved to {output_dir}")
```

### 4. Live Trading Integration

```python
# src/metalabeling/live_filter.py

class MetaLabelingFilter:
    """
    Live trading filter using trained meta-labeling models
    Integrates with existing THUNES trading system
    """

    def __init__(self, model_dir='models/metalabeling/', confidence_threshold=0.6):
        self.confidence_threshold = confidence_threshold
        self.load_models(model_dir)
        self.stats = {
            'signals_generated': 0,
            'signals_filtered': 0,
            'signals_executed': 0,
            'filter_reasons': []
        }

    def load_models(self, model_dir):
        """Load pre-trained models"""
        import joblib

        # Load models
        self.models = {}
        for model_name in ['xgboost', 'random_forest', 'logistic']:
            self.models[model_name] = joblib.load(f"{model_dir}/{model_name}_model.pkl")

        # Load preprocessors
        self.scaler = joblib.load(f"{model_dir}/scaler.pkl")
        self.meta_weights = np.load(f"{model_dir}/meta_weights.npy")

        # Load feature columns
        with open(f"{model_dir}/feature_columns.txt", 'r') as f:
            self.feature_columns = [line.strip() for line in f]

        print(f"Loaded {len(self.models)} models with {len(self.feature_columns)} features")

    def should_execute_trade(self, signal_features: dict) -> tuple:
        """
        Main decision function for live trading
        Returns: (should_trade: bool, confidence: float, reason: str)
        """
        self.stats['signals_generated'] += 1

        # Prepare features
        try:
            feature_vector = self._prepare_feature_vector(signal_features)
        except Exception as e:
            reason = f"Feature preparation error: {str(e)}"
            self.stats['filter_reasons'].append(reason)
            return False, 0.0, reason

        # Get ensemble prediction
        confidence = self._get_ensemble_confidence(feature_vector)

        # Make decision
        should_trade = confidence >= self.confidence_threshold

        if should_trade:
            self.stats['signals_executed'] += 1
            reason = f"Signal approved with confidence {confidence:.3f}"
        else:
            self.stats['signals_filtered'] += 1
            reason = f"Signal filtered - confidence {confidence:.3f} below threshold {self.confidence_threshold}"
            self.stats['filter_reasons'].append(reason)

        return should_trade, confidence, reason

    def _prepare_feature_vector(self, signal_features: dict):
        """Prepare feature vector matching training format"""
        # Ensure all features are present
        feature_vector = []
        for col in self.feature_columns:
            if col in signal_features:
                feature_vector.append(signal_features[col])
            else:
                # Use mean from training (stored in scaler)
                feature_vector.append(0)  # Will be replaced by scaler

        feature_vector = np.array(feature_vector).reshape(1, -1)
        return self.scaler.transform(feature_vector)

    def _get_ensemble_confidence(self, feature_vector):
        """Get weighted ensemble prediction"""
        predictions = []

        for name, model in self.models.items():
            pred_proba = model.predict_proba(feature_vector)[0, 1]
            predictions.append(pred_proba)

        # Weighted average
        confidence = np.average(predictions, weights=self.meta_weights)
        return confidence

    def get_performance_stats(self):
        """Return filter performance statistics"""
        total = self.stats['signals_generated']
        if total == 0:
            return {
                'filter_rate': 0,
                'execution_rate': 0,
                'total_signals': 0
            }

        return {
            'filter_rate': self.stats['signals_filtered'] / total,
            'execution_rate': self.stats['signals_executed'] / total,
            'total_signals': total,
            'filtered': self.stats['signals_filtered'],
            'executed': self.stats['signals_executed']
        }

    def adjust_confidence_threshold(self, new_threshold: float):
        """Dynamically adjust confidence threshold"""
        old_threshold = self.confidence_threshold
        self.confidence_threshold = new_threshold
        print(f"Confidence threshold adjusted: {old_threshold:.2f} -> {new_threshold:.2f}")
```

### 5. Integration with THUNES Architecture

```python
# src/live/paper_trader_with_metalabeling.py

from src.live.paper_trader import PaperTrader
from src.metalabeling.live_filter import MetaLabelingFilter
from src.metalabeling.features import MetaLabelingFeatures
import pandas as pd

class MetaLabeledPaperTrader(PaperTrader):
    """
    Enhanced PaperTrader with meta-labeling filter
    """

    def __init__(self, config, enable_metalabeling=True, confidence_threshold=0.6):
        super().__init__(config)
        self.enable_metalabeling = enable_metalabeling

        if enable_metalabeling:
            self.meta_filter = MetaLabelingFilter(confidence_threshold=confidence_threshold)
            self.feature_calculator = MetaLabelingFeatures()
            print(f"Meta-labeling enabled with confidence threshold: {confidence_threshold}")

    async def execute_strategy(self, symbol: str):
        """Override to add meta-labeling filter"""
        # Get market data
        df = await self.fetch_market_data(symbol)

        # Generate primary signal (SMA crossover)
        signal = self.strategy.generate_signal(df)

        if signal != 0:  # Primary strategy generated a signal
            if self.enable_metalabeling:
                # Calculate features
                features = self.feature_calculator.calculate_all_features(df, len(df) - 1)

                # Get meta-labeling decision
                should_trade, confidence, reason = self.meta_filter.should_execute_trade(features)

                # Log decision
                self.logger.info(f"Meta-labeling decision: {should_trade} (confidence: {confidence:.3f}) - {reason}")

                # Update audit trail
                self.audit_trail.log_event(
                    event_type="META_LABELING_DECISION",
                    details={
                        'symbol': symbol,
                        'primary_signal': signal,
                        'confidence': confidence,
                        'should_trade': should_trade,
                        'reason': reason
                    }
                )

                if not should_trade:
                    self.logger.info(f"Signal filtered by meta-labeling: {reason}")
                    return  # Skip this trade

            # Execute trade (existing logic)
            await self.execute_trade(symbol, signal)

    def get_metalabeling_stats(self):
        """Get meta-labeling performance statistics"""
        if not self.enable_metalabeling:
            return None

        stats = self.meta_filter.get_performance_stats()
        stats['threshold'] = self.meta_filter.confidence_threshold
        return stats
```

---

## Testing & Validation Plan

### 1. Backtesting Validation

```python
# tests/test_metalabeling_backtest.py

def test_metalabeling_improvement():
    """Test that meta-labeling improves key metrics"""
    # Load historical data (1+ year)
    data = load_test_data('BTCUSDT', '1h', days=365)

    # Run baseline strategy
    baseline_results = run_baseline_strategy(data)

    # Run with meta-labeling
    metalabeled_results = run_metalabeled_strategy(data)

    # Assert improvements
    assert metalabeled_results['sharpe'] > baseline_results['sharpe']
    assert metalabeled_results['max_drawdown'] < baseline_results['max_drawdown']
    assert metalabeled_results['win_rate'] > baseline_results['win_rate']

    # Expected improvements based on Reddit research
    win_rate_improvement = (metalabeled_results['win_rate'] - baseline_results['win_rate']) / baseline_results['win_rate']
    assert 0.01 <= win_rate_improvement <= 0.05  # 1-5% improvement

    drawdown_reduction = (baseline_results['max_drawdown'] - metalabeled_results['max_drawdown']) / baseline_results['max_drawdown']
    assert 0.15 <= drawdown_reduction <= 0.45  # 15-45% reduction
```

### 2. Forward Testing Protocol

```python
# Phase 15 Forward Testing Plan

FORWARD_TEST_CONFIG = {
    'duration_weeks': 4,
    'initial_confidence_threshold': 0.6,
    'adjustment_schedule': {
        'week_1': 0.7,  # Conservative start
        'week_2': 0.65,
        'week_3': 0.6,
        'week_4': 0.55  # If performing well
    },
    'success_criteria': {
        'filter_rate': (0.3, 0.5),  # 30-50% of signals filtered
        'win_rate_improvement': 0.01,  # Minimum 1%
        'drawdown_reduction': 0.15,  # Minimum 15%
        'execution_rate': 0.5  # At least 50% signals executed
    }
}
```

---

## Performance Monitoring

### Key Metrics Dashboard

```python
# src/metalabeling/monitoring.py

class MetaLabelingMonitor:
    """Real-time monitoring of meta-labeling performance"""

    def generate_report(self, start_date, end_date):
        """Generate performance report"""
        return {
            'filter_statistics': {
                'total_signals': self.total_signals,
                'filtered': self.filtered_signals,
                'executed': self.executed_signals,
                'filter_rate': self.filtered_signals / self.total_signals
            },
            'quality_metrics': {
                'true_positive_rate': self.calculate_tpr(),
                'false_positive_rate': self.calculate_fpr(),
                'precision': self.calculate_precision(),
                'confidence_distribution': self.get_confidence_histogram()
            },
            'performance_impact': {
                'win_rate_change': self.win_rate_with_filter - self.baseline_win_rate,
                'sharpe_change': self.sharpe_with_filter - self.baseline_sharpe,
                'drawdown_change': self.drawdown_with_filter - self.baseline_drawdown
            },
            'recommendations': self.generate_recommendations()
        }

    def generate_recommendations(self):
        """Generate actionable recommendations"""
        recommendations = []

        if self.filter_rate > 0.7:
            recommendations.append("Consider lowering confidence threshold - filtering too many signals")
        elif self.filter_rate < 0.2:
            recommendations.append("Consider raising confidence threshold - not filtering enough")

        if self.false_positive_rate > 0.4:
            recommendations.append("Model may need retraining - high false positive rate")

        return recommendations
```

---

## Deployment Checklist

### Pre-Deployment Requirements

- [ ] **Data Collection**
  - [ ] 1,000+ historical trades minimum
  - [ ] 5,000+ trades optimal
  - [ ] Multiple market conditions (bull/bear/sideways)

- [ ] **Model Training**
  - [ ] Cross-validation AUC > 0.65
  - [ ] Calibration verified (isotonic regression)
  - [ ] Feature importance analyzed

- [ ] **Integration Testing**
  - [ ] Paper trader integration tested
  - [ ] Audit trail logging verified
  - [ ] Risk manager compatibility confirmed

- [ ] **Performance Validation**
  - [ ] Backtested improvement verified
  - [ ] Forward test (4 weeks) completed
  - [ ] Filter rate in expected range (30-50%)

### Go-Live Criteria

1. **Minimum Performance Targets**:
   - Win rate improvement: ≥1%
   - Drawdown reduction: ≥15%
   - Sharpe improvement: ≥0.1

2. **Stability Requirements**:
   - 4 weeks paper trading with meta-labeling
   - Consistent filter rate (±10% variation)
   - No critical errors in production

3. **Monitoring in Place**:
   - Real-time dashboard operational
   - Alert thresholds configured
   - Rollback plan documented

---

## Risk Management

### Potential Failure Modes

1. **Over-filtering** (>70% signals filtered)
   - Solution: Lower confidence threshold incrementally
   - Monitoring: Daily filter rate tracking

2. **Model Degradation** (performance decline)
   - Solution: Weekly retraining schedule
   - Monitoring: Rolling 30-day performance metrics

3. **Feature Calculation Errors**
   - Solution: Fallback to primary strategy only
   - Monitoring: Feature validation checks

### Emergency Procedures

```python
# Emergency disable meta-labeling
async def emergency_disable_metalabeling():
    """Immediately disable meta-labeling and revert to baseline"""
    config.ENABLE_METALABELING = False
    await send_telegram_alert("META-LABELING DISABLED - Reverting to baseline strategy")
    audit_trail.log_critical_event("METALABELING_EMERGENCY_DISABLE")
```

---

## Success Metrics

### Week 1-2 Targets
- Filter rate: 40-60%
- Execution rate: 40-60%
- No critical errors

### Week 3-4 Targets
- Win rate improvement: ≥1%
- Drawdown reduction: ≥10%
- Sharpe improvement: ≥0.05

### Month 2-3 Targets
- Win rate improvement: 2-3%
- Drawdown reduction: 20-30%
- Sharpe improvement: 0.2-0.3

---

**Document Version**: 1.0
**Author**: THUNES Development Team (via Claude Code)
**Last Updated**: January 2025
**Next Review**: After Phase 14 Completion